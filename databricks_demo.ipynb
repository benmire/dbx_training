{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks Demo Notebook\n",
    "\n",
    "This notebook demonstrates basic Spark operations similar to Databricks notebooks.\n",
    "\n",
    "**Prerequisites:**\n",
    "- PySpark installed: `pip install pyspark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, count, when\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Databricks Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(\"✅ Spark session created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Sample Data\n",
    "\n",
    "Let's create a simple sales dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sales data\n",
    "data = [\n",
    "    (\"Alice\", \"Electronics\", 1200, 5),\n",
    "    (\"Bob\", \"Clothing\", 450, 3),\n",
    "    (\"Charlie\", \"Electronics\", 800, 2),\n",
    "    (\"Alice\", \"Clothing\", 300, 1),\n",
    "    (\"Bob\", \"Electronics\", 950, 4),\n",
    "    (\"Charlie\", \"Clothing\", 600, 2),\n",
    "    (\"Alice\", \"Electronics\", 1500, 6),\n",
    "    (\"Bob\", \"Clothing\", 350, 2),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"salesperson\", \"category\", \"amount\", \"quantity\"])\n",
    "\n",
    "print(f\"Created DataFrame with {df.count()} rows\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show schema\n",
    "print(\"DataFrame Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filter and Select Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for high-value sales (amount > 500)\n",
    "high_value_sales = df.filter(col(\"amount\") > 500)\n",
    "\n",
    "print(\"High-value sales (amount > 500):\")\n",
    "high_value_sales.show()\n",
    "\n",
    "# Select specific columns\n",
    "print(\"\\nSalesperson and Amount only:\")\n",
    "df.select(\"salesperson\", \"amount\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aggregations and Group By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total sales by salesperson\n",
    "sales_by_person = df.groupBy(\"salesperson\") \\\n",
    "    .agg(\n",
    "        sum(\"amount\").alias(\"total_sales\"),\n",
    "        count(\"*\").alias(\"num_transactions\"),\n",
    "        avg(\"amount\").alias(\"avg_sale_amount\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"total_sales\").desc())\n",
    "\n",
    "print(\"Sales Summary by Salesperson:\")\n",
    "sales_by_person.show()\n",
    "\n",
    "# Total sales by category\n",
    "sales_by_category = df.groupBy(\"category\") \\\n",
    "    .agg(\n",
    "        sum(\"amount\").alias(\"total_sales\"),\n",
    "        sum(\"quantity\").alias(\"total_quantity\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"total_sales\").desc())\n",
    "\n",
    "print(\"\\nSales Summary by Category:\")\n",
    "sales_by_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Add Calculated Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a calculated column for price per item\n",
    "df_with_price = df.withColumn(\"price_per_item\", col(\"amount\") / col(\"quantity\"))\n",
    "\n",
    "# Add a category for sale size\n",
    "df_enriched = df_with_price.withColumn(\n",
    "    \"sale_size\",\n",
    "    when(col(\"amount\") > 1000, \"Large\")\n",
    "    .when(col(\"amount\") > 500, \"Medium\")\n",
    "    .otherwise(\"Small\")\n",
    ")\n",
    "\n",
    "print(\"Enriched DataFrame with calculated columns:\")\n",
    "df_enriched.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SQL Queries\n",
    "\n",
    "You can also use SQL syntax with Spark DataFrames!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view\n",
    "df.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "# Run SQL query\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        salesperson,\n",
    "        category,\n",
    "        SUM(amount) as total_sales,\n",
    "        COUNT(*) as num_transactions\n",
    "    FROM sales\n",
    "    WHERE amount > 400\n",
    "    GROUP BY salesperson, category\n",
    "    ORDER BY total_sales DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"SQL Query Results:\")\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Convert to Pandas (Optional)\n",
    "\n",
    "You can convert small Spark DataFrames to Pandas if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas (requires pandas: pip install pandas)\n",
    "try:\n",
    "    pandas_df = sales_by_person.toPandas()\n",
    "    print(\"Converted to Pandas DataFrame:\")\n",
    "    print(pandas_df)\n",
    "except ImportError:\n",
    "    print(\"Pandas not installed - skipping conversion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results (Optional)\n",
    "\n",
    "Save the results to CSV or Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV (commented out - uncomment to run)\n",
    "# sales_by_person.write.mode('overwrite').csv('output/sales_summary.csv', header=True)\n",
    "\n",
    "# Save to Parquet (commented out - uncomment to run)\n",
    "# df_enriched.write.mode('overwrite').parquet('output/sales_enriched.parquet')\n",
    "\n",
    "print(\"✅ Notebook complete! You've successfully:\")\n",
    "print(\"  - Created a Spark session\")\n",
    "print(\"  - Created and manipulated DataFrames\")\n",
    "print(\"  - Performed aggregations and filtering\")\n",
    "print(\"  - Used SQL queries\")\n",
    "print(\"  - Worked with Spark DataFrames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session when done\n",
    "spark.stop()\n",
    "print(\"✅ Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
